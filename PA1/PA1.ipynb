{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koushikbhat/anaconda3/envs/DLI/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######## Ref : https://www.kaggle.com/code/hojjatk/read-mnist-dataset/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DLI5179/PA1/t10k-images.idx3-ubyte\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "input_path = '/home/koushikbhat/Documents/DLI5179/PA1/'\n",
    "training_images_filepath = join(input_path, 'train-images.idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels.idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images.idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels.idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:  {0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"Train labels: \", dict(zip(unique, counts)))\n",
    "np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test labels:  {0: 980, 1: 1135, 2: 1032, 3: 1010, 4: 982, 5: 892, 6: 958, 7: 1028, 8: 974, 9: 1009}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of unique test labels\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(\"\\nTest labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "np.sum(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 60000\n",
      "Number of testing examples: m_test = 10000\n",
      "Height/Width of each image: num_px = 28\n",
      "train_set_x shape: (60000, 28, 28)\n",
      "train_set_y shape: (60000,)\n",
      "test_set_x shape: (10000, 28, 28)\n",
      "test_set_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "m_train = x_train.shape[0]\n",
    "m_test =  x_test.shape[0]\n",
    "num_px = x_test.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"train_set_x shape: \" + str(x_train.shape))\n",
    "print (\"train_set_y shape: \" + str(y_train.shape))\n",
    "print (\"test_set_x shape: \" + str(x_test.shape))\n",
    "print (\"test_set_y shape: \" + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(ncols=5, sharex=False, \n",
    "    sharey=True, figsize=(10, 4))\n",
    "for i in range(5):\n",
    "    axes[i].set_title(y_train[i])\n",
    "    axes[i].imshow(x_train[i], cmap='gray')\n",
    "    axes[i].get_xaxis().set_visible(False)\n",
    "    axes[i].get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "# 0 -> (1, 0, 0, 0), 1 -> (0, 1, 0, 0), 2 -> (0, 0, 1, 0), 3 -> (0, 0, 0, 1)\n",
    "y_OH_train = enc.fit_transform(np.expand_dims(y_train,1)).toarray()\n",
    "y_OH_test = enc.fit_transform(np.expand_dims(y_test,1)).toarray()\n",
    "print(y_OH_train.shape, y_OH_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_OH_train.T\n",
    "y_test =y_OH_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert y_train into one-hot format\\nfrom tensorflow.keras.utils import to_categorical\\ntemp = []\\nfor i in range(len(y_train)):\\n    temp.append(to_categorical(y_train[i], num_classes=10))\\ny_train = np.array(temp)# Convert y_test into one-hot format\\ntemp = []\\nfor i in range(len(y_test)):    \\n    temp.append(to_categorical(y_test[i], num_classes=10))\\ny_test = np.array(temp)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Convert y_train into one-hot format\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "temp = []\n",
    "for i in range(len(y_train)):\n",
    "    temp.append(to_categorical(y_train[i], num_classes=10))\n",
    "y_train = np.array(temp)# Convert y_test into one-hot format\n",
    "temp = []\n",
    "for i in range(len(y_test)):    \n",
    "    temp.append(to_categorical(y_test[i], num_classes=10))\n",
    "y_test = np.array(temp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(10, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (60000, 784)\n",
      "test_set_x_flatten shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples\n",
    "\n",
    "train_set_x_flatten = x_train.reshape(x_train.shape[0], -1)\n",
    "test_set_x_flatten = x_test.reshape(x_test.shape[0], -1)\n",
    "\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x.transpose()\n",
    "test_set_x = test_set_x.T.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 60000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 60000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1000)\n",
      "(10, 1000)\n"
     ]
    }
   ],
   "source": [
    "train_set_x = train_set_x[:, :1000]\n",
    "print(train_set_x.shape)\n",
    "y_train = y_train[:,:1000]\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x) * (1- sigmoid(x))\n",
    "\n",
    "def grad_relu(x):\n",
    "    return x>0\n",
    "\n",
    "def derivative_of_softmax(x):\n",
    "\n",
    "    return softmax(x)*(1-softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(ni,nh):  # Glorot uniform\n",
    "  nin = ni; nout = nh\n",
    "  sd = np.sqrt(6.0 / (nin + nout))\n",
    "  #print(sd)\n",
    "  W = []\n",
    "  for i in range(ni):\n",
    "    for j in range(nh):\n",
    "      x = np.float32(np.random.uniform(-sd, sd))\n",
    "      print(x)\n",
    "      W.append(x)\n",
    "  return W\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I/P → h 1 (500) → h 2 (250) → h 3 (100) → O/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef initialize_parameters(n_x, n_h1,n_h2,n_h3, n_y):\\n    W1 = init_weights(n_h1, n_x)\\n    W2 = init_weights(n_h2, n_h1)\\n    W3 = init_weights(n_h3, n_h2)\\n    W4 = init_weights(n_y, n_h3)\\n    b1 = np.zeros((n_h1, 1))\\n    b2 = np.zeros((n_h2, 1))\\n    b3 = np.zeros((n_h3, 1))\\n    b4 = np.zeros((n_y, 1))\\n    return W1, W2, W3, W4 ,b1, b2, b3 ,b4\\n    '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "def initialize_parameters(n_x, n_h1,n_h2,n_h3, n_y):\n",
    "    W1 = init_weights(n_h1, n_x)\n",
    "    W2 = init_weights(n_h2, n_h1)\n",
    "    W3 = init_weights(n_h3, n_h2)\n",
    "    W4 = init_weights(n_y, n_h3)\n",
    "    b1 = np.zeros((n_h1, 1))\n",
    "    b2 = np.zeros((n_h2, 1))\n",
    "    b3 = np.zeros((n_h3, 1))\n",
    "    b4 = np.zeros((n_y, 1))\n",
    "    return W1, W2, W3, W4 ,b1, b2, b3 ,b4\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "                    W4\n",
    "                    b4\n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "        \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    W4 = parameters[\"W4\"]\n",
    "    b4 = parameters[\"b4\"]\n",
    "\n",
    "    print(\"input X \", X.shape)\n",
    "    print(\"W1\",W1.shape)\n",
    "    print(\"b1\",b1.shape)\n",
    "    print(\"W2\",W2.shape)\n",
    "    print(\"b2\",b2.shape)\n",
    "    print(\"W3\",W3.shape)\n",
    "    print(\"b3\",b3.shape)\n",
    "    print(\"W4\",W4.shape)\n",
    "    print(\"b4\",b4.shape)\n",
    "\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    print(\"Z1\",Z1.shape)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    print(\"Z2\",Z2.shape)\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    print(\"Z3\",Z3.shape)\n",
    "    A3 = sigmoid(Z3)\n",
    "    print(\"A3\",A3.shape)\n",
    "    Z4 = np.dot(W4, A3) + b4\n",
    "    print(\"Z4\",Z4.shape)\n",
    "    A4 = softmax(Z4)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3, Z4 , A4 ,W4 ,b4)\n",
    "    \n",
    "    return A4, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3 ,Z4 , A4 ,W4 ,b4 ) = cache\n",
    "    print(\"W1\",W1.shape)\n",
    "    print(\"b1\",b1.shape)\n",
    "    print(\"W2\",W2.shape)\n",
    "    print(\"b2\",b2.shape)\n",
    "    print(\"W3\",W3.shape)\n",
    "    print(\"b3\",b3.shape)\n",
    "    print(\"W4\",W4.shape)\n",
    "    print(\"b4\",b4.shape)\n",
    "    print(\"A1\",A1.shape)\n",
    "    print(\"A2\",A2.shape)\n",
    "    print(\"A3\",A3.shape)\n",
    "    print(\"A4\",A4.shape)\n",
    "    \n",
    "    \"\"\"W1 (500, 784)\n",
    "            b1 (500, 1)\n",
    "            W2 (250, 500)\n",
    "            b2 (250, 1)\n",
    "            W3 (100, 250)\n",
    "            b3 (100, 1)\n",
    "            W4 (10, 1000) #error\n",
    "            b4 (10, 1000)\n",
    "            A1 (500, 1000)\n",
    "            A2 (250, 1000)\n",
    "            A3 (100, 1000)\n",
    "            A4 (10, 1)\n",
    "\n",
    "dW4 (10, 100)\n",
    "dZ4 (10, 1000)\n",
    "db4(10, 1)\n",
    "dA3 (1000, 1000)\"\"\"\n",
    "\n",
    "    dZ4 = A4 - Y\n",
    "    dW4 = 1./m * np.dot(dZ4, A3.T)\n",
    "    db4 = 1./m * np.sum(dZ4, axis=1, keepdims = True)\n",
    "    print(\"dW4\",dW4.shape)\n",
    "    print(\"dZ4\",dZ4.shape)\n",
    "    print(\"db4\",db4.shape)\n",
    "    \n",
    "    dA3 = np.dot(W4.T, dZ4)\n",
    "    dZ3 = np.multiply(dA3, np.int64(A3 > 0))\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    print(\"dA3\",dA3.shape)\n",
    "    print(\"dZ3\",dZ3.shape)\n",
    "    print(\"dW3\",dW3.shape)\n",
    "    print(\"db3\",db3.shape)\n",
    "\n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    print(\"dA2\",dA2.shape)\n",
    "    print(\"dZ2\",dZ2.shape)\n",
    "    print(\"dW2\",dW2.shape)\n",
    "    print(\"db2\",db2.shape)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    print(\"dA1\",dA1.shape)\n",
    "    print(\"dZ1\",dZ1.shape)\n",
    "    print(\"dW1\",dW1.shape)\n",
    "    print(\"db1\",db1.shape)\n",
    "    \n",
    "    gradients = {\"dZ4\": dZ4, \"dW4\": dW4, \"db4\": db4,\n",
    "                 \"dA3\": dA3, \"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(i)] = Wi\n",
    "                    parameters['b' + str(i)] = bi\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(i)] = dWi\n",
    "                    grads['db' + str(i)] = dbi\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    \n",
    "    n = len(parameters) // 2 # number of layers in the neural networks\n",
    "\n",
    "    # Update rule for each parameter\n",
    "    for k in range(n):\n",
    "        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n",
    "        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a4, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a4.shape[1]):\n",
    "        if a4[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "\n",
    "    #print (\"predictions: \" + str(p[0,:]))\n",
    "    #print (\"true labels: \" + str(y[0,:]))\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(a4, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    \n",
    "    Arguments:\n",
    "    a4 -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, same shape as a4\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the cost function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs = np.multiply(-np.log(a4),Y) + np.multiply(-np.log(1 - a4), 1 - Y)\n",
    "    cost = 1./m * np.nansum(logprobs)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "a4 (10, 1000)\n"
     ]
    }
   ],
   "source": [
    "grads = {}\n",
    "costs = []                            # to keep track of the cost\n",
    "m = train_set_x[0]                        # number of examples\n",
    "layers_dims = [train_set_x.shape[0], 500, 250, 100,10]\n",
    "\n",
    "\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "a4, cache = forward_propagation(train_set_x, parameters)\n",
    "print('a4',a4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = compute_cost(a4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(train_set_x,y_train, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, learning_rate = 0.001, num_iterations = 10, print_cost = True, lambd = 0, keep_prob = 1):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- If True, print the cost every 10000 iterations\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learned by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = []                            # to keep track of the cost\n",
    "    m = X.shape[1]                        # number of examples\n",
    "    layers_dims = [X.shape[0], 500, 250, 100,10]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    a4, cache = forward_propagation(X, parameters)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        if keep_prob == 1:\n",
    "            a4, cache = forward_propagation(X, parameters)\n",
    "        elif keep_prob < 1:\n",
    "            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n",
    "        \n",
    "        # Cost function\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(a4, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, \n",
    "                                            # but this assignment will only explore one at a time\n",
    "        if lambd == 0 and keep_prob == 1:\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "        elif lambd != 0:\n",
    "            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n",
    "        elif keep_prob < 1:\n",
    "            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 10000 iterations\n",
    "        if print_cost and i % 10000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (x1,000)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.geeksforgeeks.org/ml-mini-batch-gradient-descent-with-python/ \n",
    "# use this to create mini batches and run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "Cost after iteration 0: 3.3480753501027922\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "A1 (500, 1000)\n",
      "A2 (250, 1000)\n",
      "A3 (100, 1000)\n",
      "A4 (10, 1000)\n",
      "dW4 (10, 100)\n",
      "dZ4 (10, 1000)\n",
      "db4 (10, 1)\n",
      "dA3 (100, 1000)\n",
      "dZ3 (100, 1000)\n",
      "dW3 (100, 250)\n",
      "db3 (100, 1)\n",
      "dA2 (250, 1000)\n",
      "dZ2 (250, 1000)\n",
      "dW2 (250, 500)\n",
      "db2 (250, 1)\n",
      "dA1 (500, 1000)\n",
      "dZ1 (500, 1000)\n",
      "dW1 (500, 784)\n",
      "db1 (500, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the training set:\n",
      "input X  (784, 1000)\n",
      "W1 (500, 784)\n",
      "b1 (500, 1)\n",
      "W2 (250, 500)\n",
      "b2 (250, 1)\n",
      "W3 (100, 250)\n",
      "b3 (100, 1)\n",
      "W4 (10, 100)\n",
      "b4 (10, 1)\n",
      "Z1 (500, 1000)\n",
      "Z2 (250, 1000)\n",
      "Z3 (100, 1000)\n",
      "A3 (100, 1000)\n",
      "Z4 (10, 1000)\n",
      "Accuracy: 0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41478/2396437680.py:14: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  p = np.zeros((1,m), dtype = np.int)\n"
     ]
    }
   ],
   "source": [
    "parameters = model(train_set_x, y_train)\n",
    "print (\"On the training set:\")\n",
    "predictions_train = predict(train_set_x, y_train, parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DLI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "00a1ae1e6deaa37076d141dc26cd1e058ef740ecaf1a0d690f5f0ccaa51e4cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
