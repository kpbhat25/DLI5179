{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import flatten \n",
    "from sklearn.metrics import classification_report #for detailed statistics on classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.datasets import MNIST #importing the MNIST Dataset already there on torch\n",
    "from tqdm import tqdm \n",
    "from torchvision import transforms #for transforming the training and testing data \n",
    "from torch.utils.data import DataLoader #Dataloader loads the data batchwise with shuffling in a hassle free manner\n",
    "from torch.optim import Adam #Adam for GD\n",
    "import time \n",
    "from torchvision.utils import make_grid #to visualize the kernels the tensors\n",
    "from torch.autograd import Variable #For the adversarial examples part to find the gradients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module): \n",
    "    def __init__(self): \n",
    "        super(CNN,self).__init__()\n",
    "        self.l1 = nn.Sequential(nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                 nn.ReLU(), \n",
    "                                 nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))\n",
    "                                 #https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "                                 #for maxpool one can use a single number for a square windown, but to maintain the uniformity we are giving this way\n",
    "        \n",
    "        self.l2 = nn.Sequential(nn.Conv2d(32, 32, kernel_size = 3, stride = 1, padding = 1),\n",
    "                                 nn.ReLU(), \n",
    "                                 nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)))  \n",
    "        \n",
    "        self.fully_connected_l1 = nn.Sequential(nn.Linear(7*7*32, 500),nn.ReLU())\n",
    "        self.fully_connected_l2 = nn.Linear(500,10)\n",
    "        self.logsoftmax = nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self,x): #defines the structure of the network thus help in performing backprop, it does forward pass\n",
    "        \n",
    "        output = self.l1(x.float()) #converting x to float datatype\n",
    "        output = self.l2(output)\n",
    "        output = flatten(output,1)\n",
    "        output = self.fully_connected_l1(output)\n",
    "        output = self.fully_connected_l2(output)\n",
    "        \n",
    "        prediction   = self.logsoftmax(output)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the constants/parameters \n",
    "batch_size = 64\n",
    "number_of_epochs = 3\n",
    "learning_rate = 1e-3\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIRECTORY = os.getcwd()\n",
    "FOLDER_NAME = 'mnist'\n",
    "MODEL_FOLDER = 'model'\n",
    "save_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "if not os.path.exists(os.path.join(CURRENT_DIRECTORY, FOLDER_NAME)):\n",
    "    os.mkdir(os.path.join(CURRENT_DIRECTORY, FOLDER_NAME))\n",
    "    to_download = True\n",
    "if (save_model):\n",
    "    if not os.path.exists(os.path.join(CURRENT_DIRECTORY, MODEL_FOLDER)):\n",
    "        os.mkdir(os.path.join(CURRENT_DIRECTORY, MODEL_FOLDER))\n",
    "PATH_TO_STORE_MODEL = os.path.join(CURRENT_DIRECTORY, MODEL_FOLDER)+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #checks for gpu else runs in cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_model(model,device,Train_Data_Loader,optimizer,loss_function,train_length):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    training_loss = 0\n",
    "    training_predictions = 0\n",
    "    \n",
    "    for (data,label) in tqdm(Train_Data_Loader):\n",
    "        #sending botht the image array and labels from the dataloader to the model trainer\n",
    "\n",
    "        (data,label) = (data.to(device),label.to(device))\n",
    "        #sending the data and the labels into the gpu \n",
    "\n",
    "        prediction = model(data) #predict in the forward pass\n",
    "        loss = loss_function(prediction,label) # calculate the loss \n",
    "\n",
    "        training_loss += loss\n",
    "        training_predictions += (prediction.argmax(1) == label).type(torch.float).sum().item()\n",
    "        #prediction from the forward pass == labels , then the count of it will increase\n",
    "\n",
    "    train_accuracy = training_predictions/train_length\n",
    "\n",
    "    return training_loss , train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Test_model(model,device,Test_Data_Loader,loss_function,test_length):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    testing_loss = 0\n",
    "    testing_predictions = 0\n",
    "    \n",
    "    for (data,label) in tqdm(Test_Data_Loader):\n",
    "        #sending botht the image array and labels from the dataloader to the model trainer\n",
    "\n",
    "        (data,label) = (data.to(device),label.to(device))\n",
    "        #sending the data and the labels into the gpu \n",
    "\n",
    "        prediction = model(data) #predict in the forward pass\n",
    "        loss = loss_function(prediction,label) # calculate the loss \n",
    "\n",
    "        testing_loss += loss\n",
    "        testing_predictions += (prediction.argmax(1) == label).type(torch.float).sum().item()\n",
    "        #prediction from the forward pass == labels , then the count of it will increase\n",
    "\n",
    "    test_accuracy = testing_predictions/test_length\n",
    "\n",
    "    return testing_loss , test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "    \n",
    "    #organize the training and test data\n",
    "    \n",
    "train_data    = MNIST(CURRENT_DIRECTORY, train = True, download = True, transform = app_transform) #getting training data\n",
    "test_data     = MNIST(CURRENT_DIRECTORY, train = False, transform = app_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Data_Loader = DataLoader(train_data, batch_size = batch_size, shuffle = True ) \n",
    "Test_Data_Loader  = DataLoader(test_data, batch_size = batch_size) \n",
    "\n",
    "train_length  = len(Train_Data_Loader.dataset) #no of training examples\n",
    "test_length   = len(Test_Data_Loader.dataset)  #no of testing cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"0\",\n",
    "    1: \"1\",\n",
    "    2: \"2\",\n",
    "    3: \"3\",\n",
    "    4: \"4\",\n",
    "    5: \"5\",\n",
    "    6: \"6\",\n",
    "    7: \"7\",\n",
    "    8: \"8\",\n",
    "    9: \"9\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(Train_Data_Loader.dataset), size=(1,)).item()\n",
    "    img, label = Train_Data_Loader.dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),lr = learning_rate) #optimizer\n",
    "    \n",
    "lossfn    = nn.NLLLoss() #lossFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "l1.0.weight \t torch.Size([32, 1, 3, 3])\n",
      "l1.0.bias \t torch.Size([32])\n",
      "l2.0.weight \t torch.Size([32, 32, 3, 3])\n",
      "l2.0.bias \t torch.Size([32])\n",
      "fully_connected_l1.0.weight \t torch.Size([500, 1568])\n",
      "fully_connected_l1.0.bias \t torch.Size([500])\n",
      "fully_connected_l2.weight \t torch.Size([10, 500])\n",
      "fully_connected_l2.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7]}]\n"
     ]
    }
   ],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/koushikbhat/Documents/DLI5179/PA2/EE21S055.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/koushikbhat/Documents/DLI5179/PA2/EE21S055.ipynb#X31sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m validationaccuracy_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/koushikbhat/Documents/DLI5179/PA2/EE21S055.ipynb#X31sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number_of_epochs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/koushikbhat/Documents/DLI5179/PA2/EE21S055.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m Train_Data_Loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/koushikbhat/Documents/DLI5179/PA2/EE21S055.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m       outputs \u001b[39m=\u001b[39m model1(images\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/koushikbhat/Documents/DLI5179/PA2/EE21S055.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m       loss \u001b[39m=\u001b[39m criterion1(outputs, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/DLI/lib/python3.9/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/DLI/lib/python3.9/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/DLI/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/DLI/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/DLI/lib/python3.9/site-packages/torchvision/datasets/mnist.py:138\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, Any]:\n\u001b[1;32m    131\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[39m        index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39m        tuple: (image, target) where target is index of the target class.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     img, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtargets[index])\n\u001b[1;32m    140\u001b[0m     \u001b[39m# doing this so that it is consistent with all other datasets\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# to return a PIL Image\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model1=CNN()\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "trainingloss_list = []\n",
    "validationloss_list = []\n",
    "validationaccuracy_list = []\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "  for images, labels in Train_Data_Loader:\n",
    "      outputs = model1(images.float())\n",
    "      loss = criterion1(outputs, labels)\n",
    "      trainingloss_list.append(loss.item())\n",
    "\n",
    "      optimizer1.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer1.step()\n",
    "\n",
    "  iteration=0\n",
    "  tempvalloss=0\n",
    "  correctval=0    \n",
    "  \n",
    "  for images, labels in Test_Data_Loader:\n",
    "    outputs = model1(images.float())\n",
    "    loss = criterion1(outputs, labels)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    correctval += (predicted == labels).sum().item()\n",
    "    iteration+=1\n",
    "    tempvalloss+=loss.item()\n",
    "            \n",
    "  validationloss_list.append(tempvalloss/iteration)\n",
    "  validationaccuracy_list.append(correctval/100)\n",
    "  print('Epoch',epoch+1,'out of',number_of_epochs,'epochs are over')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DLI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00a1ae1e6deaa37076d141dc26cd1e058ef740ecaf1a0d690f5f0ccaa51e4cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
