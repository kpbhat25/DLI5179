{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd0950ag4NNZ"
   },
   "source": [
    "### Task 03:\n",
    "\n",
    "In this task, you have to implement the Backpropagation method using Pytorch. This is particularly useful when the hypothesis function contains several weights.\n",
    "\n",
    "**Backpropagation**: Algorithm to caculate gradient for all the weights in the network with several weights. \n",
    "\n",
    "* It uses the `Chain Rule` to calcuate the gradient for multiple nodes at the same time. \n",
    "* In pytorch this is implemented using a `variable` data type and `loss.backward()` method to get the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2w0MM9204NNh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koushik/anaconda3/envs/kpb/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# import the necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st0vmvel4NNn"
   },
   "source": [
    "## Preliminaries - Pytorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BUgr0pR4NNq",
    "outputId": "f61783ca-70c0-4085-e209-3de30bf28733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([-0.6173,  0.8696, -0.0281,  0.3251, -0.8105])\n",
      "The size of the to_tensor:  torch.Size([5])\n",
      "The size of the to_tensor:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor\n",
    "\n",
    "# zero tensor\n",
    "zeros = torch.zeros(5)\n",
    "print(zeros)\n",
    "# ones\n",
    "ones = torch.ones(5)\n",
    "print(ones)\n",
    "# random normal\n",
    "random = torch.randn(5)\n",
    "print(random)\n",
    "\n",
    "\n",
    "# creating tensors from list and/or numpy arrays\n",
    "my_list = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "to_tensor = torch.Tensor(my_list)\n",
    "print(\"The size of the to_tensor: \", to_tensor.size())\n",
    "\n",
    "my_array = np.array(my_list) # or\n",
    "to_tensor = torch.tensor(my_array)\n",
    "to_tensor = torch.from_numpy(my_array)\n",
    "print(\"The size of the to_tensor: \", to_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Js4p4btA4VvG",
    "outputId": "0dded7c7-ce10-465d-a38a-f1fdc57dc32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1051, -0.9893, -0.9709],\n",
      "        [ 1.1245, -1.9685, -0.2387],\n",
      "        [ 0.7444,  0.8782, -1.9104]])\n",
      "tensor([[[ 1.1167,  0.7769,  1.2348],\n",
      "         [-0.8657, -0.7370, -0.6928],\n",
      "         [ 0.8738,  0.1863,  0.0636]],\n",
      "\n",
      "        [[-0.9268,  1.0652,  0.3860],\n",
      "         [-0.9666,  0.2789,  0.5441],\n",
      "         [-0.1008,  1.4361,  0.1923]],\n",
      "\n",
      "        [[-0.3839,  0.0357, -1.1384],\n",
      "         [-0.7892,  0.6098, -1.4819],\n",
      "         [ 1.0663,  0.4981,  1.0752]]])\n"
     ]
    }
   ],
   "source": [
    "# multi dimenstional tensors\n",
    "\n",
    "# 2D\n",
    "two_dim = torch.randn((3, 3))\n",
    "print(two_dim)\n",
    "# 3D \n",
    "three_dim = torch.randn((3, 3, 3))\n",
    "print(three_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRA0Icia4ani",
    "outputId": "417b5749-4ed9-4a7f-9a8c-73fda0cdccd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([-1.1051,  1.1245,  0.7444])\n",
      "tensor([-1.1051, -0.9893, -0.9709])\n"
     ]
    }
   ],
   "source": [
    "# tensor shapes and axes\n",
    "\n",
    "print(zeros.shape)\n",
    "print(two_dim.shape)\n",
    "print(three_dim.shape)\n",
    "\n",
    "# zeroth axis - rows\n",
    "print(two_dim[:, 0])\n",
    "# first axis - columns\n",
    "print(two_dim[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CFwt4R_4iEq",
    "outputId": "0f4c05c4-3aae-4f5b-88e0-9d4f355d4f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1051, -0.9893],\n",
      "        [ 1.1245, -1.9685],\n",
      "        [ 0.7444,  0.8782]])\n",
      "tensor([[-1.1051, -0.9893, -0.9709],\n",
      "        [ 1.1245, -1.9685, -0.2387]])\n"
     ]
    }
   ],
   "source": [
    "print(two_dim[:, 0:2])\n",
    "print(two_dim[0:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJA1f5_F5Acn",
    "outputId": "ac3b4415-a3c6-4869-f2f5-f189af30c858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Shape :  torch.Size([2, 3])\n",
      "Resized Tensor Shape :  torch.Size([3, 2])\n",
      "Resized Tensor Shape :  torch.Size([3, 2])\n",
      "Flattened Tensor Shape :  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.randn(2,3)\n",
    "print(\"Tensor Shape : \" , rand_tensor.shape)\n",
    "resized_tensor = rand_tensor.reshape(3,2)\n",
    "print(\"Resized Tensor Shape : \" , resized_tensor.shape) # or\n",
    "resized_tensor = rand_tensor.reshape(3,-1)\n",
    "print(\"Resized Tensor Shape : \" , resized_tensor.shape)\n",
    "flattened_tensor = rand_tensor.reshape(-1)\n",
    "print(\"Flattened Tensor Shape : \" , flattened_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ismMPyM4NNw"
   },
   "source": [
    "Determine the derivative of $y = 2x^{3} + x$ at $x = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ic-ZkAPC4NNx",
    "outputId": "4e7597fc-5f5c-46d1-9f3c-5af0f4d276f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Y at x=1 :  tensor(3., grad_fn=<AddBackward0>)\n",
      "Derivative of Y wrt x at x=1 :  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = 2 * (x ** 3) + x\n",
    "y.backward()\n",
    "print(\"Value of Y at x=1 : \" , y)\n",
    "print(\"Derivative of Y wrt x at x=1 : \" , x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp-J-JDA4NNy"
   },
   "source": [
    "### Task 03 - a\n",
    "Determine the partial derivative of $y = uv + u^{2}$ at $u=1$ and $v=2$ with respect to $u$ and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "N7ShF6wf4NNz"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "u = torch.tensor(1.0, requires_grad = True)\n",
    "v = torch.tensor(2.0, requires_grad = True)\n",
    "y = u*v + u**2\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of y at u=1, v=2 :  tensor(3., grad_fn=<AddBackward0>)\n",
      "Partial Derivative of y wrt u :  tensor(4.)\n",
      "Partial Derivative of y wrt v :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# YOUR CODE ends HERE\n",
    "print(\"Value of y at u=1, v=2 : \" , y)\n",
    "print(\"Partial Derivative of y wrt u : \" , u.grad)\n",
    "print(\"Partial Derivative of y wrt v : \" , v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jvHyKE4NN0"
   },
   "source": [
    "#### Hypothesis Function and Loss Function\n",
    "\n",
    "$y = x * w + b$\n",
    "\n",
    "$loss =(\\hat{y}-y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7dU0O9E4NN0"
   },
   "source": [
    "Let us make use of a randomly-created sample dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6tUXuGub4NN1"
   },
   "outputs": [],
   "source": [
    "#sample-dataset\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI84dQfW4NN1"
   },
   "source": [
    "## Task: 03 - b\n",
    "Declare pytorch tensors for weight and bias and implement the forward and loss function of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZQaYqClT4NN1"
   },
   "outputs": [],
   "source": [
    "# Define w = 1 and b = -1 for y = wx + b\n",
    "# Note that w,b are learnable paramteter \n",
    "# i.e., you are going to take the derivative of the tensor(s).\n",
    "# YOUR CODE STARTS HERE\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "b = torch.tensor(-1.0, requires_grad = True)\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "assert w.item() == 1\n",
    "assert b.item() == -1\n",
    "assert w.requires_grad == True\n",
    "assert b.requires_grad == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zRVK-4tr4NN3"
   },
   "outputs": [],
   "source": [
    "#forward function to calculate y_pred for a given x according to the linear model defined above\n",
    "def forward(x):\n",
    "    #implement the forward model to compute y_pred as w*x + b\n",
    "    ## YOUR CODE STARTS HERE\n",
    "    return ((w*x) + b)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE\n",
    "\n",
    "#loss-function to compute the mean-squared error between y_pred and y_actual\n",
    "def loss(y_pred, y_actual):\n",
    "    #calculate the mean-squared-error between y_pred and y_actual\n",
    "    ## YOUR CODE STARTS HERE\n",
    "    return ((y_pred- y_actual)**2)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3BAF5aQ4NN3"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=4$ without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EPAgSpvE4NN4"
   },
   "outputs": [],
   "source": [
    "y_pred_without_train = forward(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BO6QOWq4NN4"
   },
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VUgLcPrG4NN4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 8.32815933227539 | w: 1.368575930595398\n",
      "Epoch: 2 | Loss: 4.635132312774658 | w: 1.641068696975708\n",
      "Epoch: 3 | Loss: 2.6127521991729736 | w: 1.842525839805603\n",
      "Epoch: 4 | Loss: 1.5045195817947388 | w: 1.991465449333191\n",
      "Epoch: 5 | Loss: 0.8966817855834961 | w: 2.1015784740448\n",
      "Epoch: 6 | Loss: 0.5628984570503235 | w: 2.182986259460449\n",
      "Epoch: 7 | Loss: 0.3793121576309204 | w: 2.2431719303131104\n",
      "Epoch: 8 | Loss: 0.2781200110912323 | w: 2.287667751312256\n",
      "Epoch: 9 | Loss: 0.22218374907970428 | w: 2.3205642700195312\n",
      "Epoch: 10 | Loss: 0.19114667177200317 | w: 2.3448848724365234\n"
     ]
    }
   ],
   "source": [
    "# In this method, we learn the dataset multiple times (called epochs)\n",
    "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
    "\n",
    "alpha = 0.01 # Let us set learning rate as 0.01\n",
    "weight_list = []\n",
    "loss_list=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in zip(x_data, y_data):\n",
    "        \n",
    "        #implement forward pass, compute loss and gradients for the weights and update weights\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        yPred = forward(x)            #forward\n",
    "        currentLoss = loss(yPred,y)     #loss\n",
    "        total_loss += currentLoss\n",
    "        currentLoss.backward()    #back\n",
    "\n",
    "        w.data = w.data - alpha * (w.grad.item()) #iterate/update weight\n",
    "        ## YOUR CODE ENDS HERE\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    avg_mse = total_loss / count        \n",
    "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w: {w.item()}\")\n",
    "    weight_list.append(w)\n",
    "    loss_list.append(avg_mse)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vfWzjw14NN5"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=4$ after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "KiQzppwU4NN6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Y Value for x=4 : 8\n",
      "Predicted Y Value before training :  3.0\n",
      "Predicted Y Value after training :  8.379539489746094\n"
     ]
    }
   ],
   "source": [
    "y_pred_with_train = forward(4)\n",
    "\n",
    "print(\"Actual Y Value for x=4 : 8\")\n",
    "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
    "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfsMg3xe4NN6"
   },
   "source": [
    "## Task: 03 - c\n",
    "Repeat **Task:03 - b** for the quadratic model defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvN4ZfIt4NN6"
   },
   "source": [
    "#### Using backward propagation for quadratic model\n",
    "\n",
    "$\\hat{y} = x^2*w_{2} + x*w_{1}$\n",
    "\n",
    "$loss = (\\hat{y}-y)^2$\n",
    "\n",
    "* Using Dummy values of x and y\n",
    "\n",
    "`x = 1,2,3,4,5`\n",
    "`y = 1,6,15,28,45`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "pKxTLgYc4NN7"
   },
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "y_data = [1.0, 6.0, 15.0, 28, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0tvvovED4NN7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the given dataset\n",
    "plt.plot(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nodeeWDX4NN7"
   },
   "outputs": [],
   "source": [
    "# Initialize w2 and w1 with randon values\n",
    "w_1 = torch.tensor([1.0], requires_grad=True)\n",
    "w_2 = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "4DZxKgss4NN7"
   },
   "outputs": [],
   "source": [
    "# Quadratic forward pass based on the function above. Taking b as zero for now\n",
    "def quad_forward(x):\n",
    "    return w_2*(x**2)+w_1*x\n",
    "\n",
    "# Loss fucntion as per the defination above\n",
    "def loss(y_pred,y_actual):\n",
    "    return (y_pred-y_actual)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKlMp8sW4NN8"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=6$ without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YiVbYy1P4NN8"
   },
   "outputs": [],
   "source": [
    "y_pred_without_train = quad_forward(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfUGvo2F4NN8"
   },
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "04CX2FJX4NN9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 23.789419174194336 | w_1: 1.1494112014770508 | w_2: 1.6618750095367432\n",
      "Epoch: 2 | Loss: 5.293725967407227 | w_1: 1.1156214475631714 | w_2: 1.5936779975891113\n",
      "Epoch: 3 | Loss: 4.067604064941406 | w_1: 1.101760745048523 | w_2: 1.6043506860733032\n",
      "Epoch: 4 | Loss: 4.0941009521484375 | w_1: 1.0858769416809082 | w_2: 1.606476068496704\n",
      "Epoch: 5 | Loss: 4.023508071899414 | w_1: 1.0703413486480713 | w_2: 1.6095004081726074\n",
      "Epoch: 6 | Loss: 3.9647536277770996 | w_1: 1.0548969507217407 | w_2: 1.612403392791748\n",
      "Epoch: 7 | Loss: 3.90571928024292 | w_1: 1.0395704507827759 | w_2: 1.6152952909469604\n",
      "Epoch: 8 | Loss: 3.8476860523223877 | w_1: 1.0243579149246216 | w_2: 1.6181646585464478\n",
      "Epoch: 9 | Loss: 3.7905020713806152 | w_1: 1.0092588663101196 | w_2: 1.6210126876831055\n",
      "Epoch: 10 | Loss: 3.734168291091919 | w_1: 0.9942724704742432 | w_2: 1.6238394975662231\n",
      "Epoch: 11 | Loss: 3.6786715984344482 | w_1: 0.9793977737426758 | w_2: 1.6266450881958008\n",
      "Epoch: 12 | Loss: 3.6239993572235107 | w_1: 0.9646340608596802 | w_2: 1.629429817199707\n",
      "Epoch: 13 | Loss: 3.570141315460205 | w_1: 0.9499804377555847 | w_2: 1.632193922996521\n",
      "Epoch: 14 | Loss: 3.517085313796997 | w_1: 0.9354360699653625 | w_2: 1.6349371671676636\n",
      "Epoch: 15 | Loss: 3.4648163318634033 | w_1: 0.9210003614425659 | w_2: 1.6376601457595825\n",
      "Epoch: 16 | Loss: 3.4133172035217285 | w_1: 0.9066721796989441 | w_2: 1.6403625011444092\n",
      "Epoch: 17 | Loss: 3.362590789794922 | w_1: 0.8924509882926941 | w_2: 1.6430450677871704\n",
      "Epoch: 18 | Loss: 3.312619686126709 | w_1: 0.8783357739448547 | w_2: 1.645707607269287\n",
      "Epoch: 19 | Loss: 3.263388156890869 | w_1: 0.8643258213996887 | w_2: 1.6483498811721802\n",
      "Epoch: 20 | Loss: 3.2148895263671875 | w_1: 0.8504204750061035 | w_2: 1.6509729623794556\n",
      "Epoch: 21 | Loss: 3.1671102046966553 | w_1: 0.8366188406944275 | w_2: 1.653576135635376\n",
      "Epoch: 22 | Loss: 3.1200389862060547 | w_1: 0.8229200839996338 | w_2: 1.6561598777770996\n",
      "Epoch: 23 | Loss: 3.07366943359375 | w_1: 0.8093234896659851 | w_2: 1.6587246656417847\n",
      "Epoch: 24 | Loss: 3.0279903411865234 | w_1: 0.7958282828330994 | w_2: 1.661270022392273\n",
      "Epoch: 25 | Loss: 2.98298978805542 | w_1: 0.7824338674545288 | w_2: 1.6637965440750122\n",
      "Epoch: 26 | Loss: 2.9386563301086426 | w_1: 0.7691393494606018 | w_2: 1.666304111480713\n",
      "Epoch: 27 | Loss: 2.8949832916259766 | w_1: 0.7559439539909363 | w_2: 1.6687930822372437\n",
      "Epoch: 28 | Loss: 2.8519604206085205 | w_1: 0.7428469657897949 | w_2: 1.671263337135315\n",
      "Epoch: 29 | Loss: 2.809572696685791 | w_1: 0.7298476696014404 | w_2: 1.673715353012085\n",
      "Epoch: 30 | Loss: 2.767817974090576 | w_1: 0.716945230960846 | w_2: 1.6761490106582642\n",
      "Epoch: 31 | Loss: 2.726682662963867 | w_1: 0.7041390538215637 | w_2: 1.678564429283142\n",
      "Epoch: 32 | Loss: 2.686161756515503 | w_1: 0.6914284229278564 | w_2: 1.6809619665145874\n",
      "Epoch: 33 | Loss: 2.646240472793579 | w_1: 0.678812563419342 | w_2: 1.6833417415618896\n",
      "Epoch: 34 | Loss: 2.6069116592407227 | w_1: 0.666290819644928 | w_2: 1.6857032775878906\n",
      "Epoch: 35 | Loss: 2.568166494369507 | w_1: 0.6538625955581665 | w_2: 1.6880476474761963\n",
      "Epoch: 36 | Loss: 2.530001163482666 | w_1: 0.6415269374847412 | w_2: 1.6903743743896484\n",
      "Epoch: 37 | Loss: 2.4924018383026123 | w_1: 0.6292833685874939 | w_2: 1.6926838159561157\n",
      "Epoch: 38 | Loss: 2.4553608894348145 | w_1: 0.6171311140060425 | w_2: 1.6949760913848877\n",
      "Epoch: 39 | Loss: 2.418872833251953 | w_1: 0.6050694584846497 | w_2: 1.6972512006759644\n",
      "Epoch: 40 | Loss: 2.3829216957092285 | w_1: 0.5930976867675781 | w_2: 1.6995090246200562\n",
      "Epoch: 41 | Loss: 2.347506523132324 | w_1: 0.5812153220176697 | w_2: 1.7017505168914795\n",
      "Epoch: 42 | Loss: 2.3126184940338135 | w_1: 0.5694215297698975 | w_2: 1.703974962234497\n",
      "Epoch: 43 | Loss: 2.2782483100891113 | w_1: 0.557715654373169 | w_2: 1.706182837486267\n",
      "Epoch: 44 | Loss: 2.2443912029266357 | w_1: 0.5460972785949707 | w_2: 1.7083745002746582\n",
      "Epoch: 45 | Loss: 2.2110342979431152 | w_1: 0.5345653891563416 | w_2: 1.7105494737625122\n",
      "Epoch: 46 | Loss: 2.1781725883483887 | w_1: 0.5231196284294128 | w_2: 1.7127083539962769\n",
      "Epoch: 47 | Loss: 2.145801544189453 | w_1: 0.5117591619491577 | w_2: 1.7148512601852417\n",
      "Epoch: 48 | Loss: 2.1139135360717773 | w_1: 0.5004834532737732 | w_2: 1.7169781923294067\n",
      "Epoch: 49 | Loss: 2.0824952125549316 | w_1: 0.48929181694984436 | w_2: 1.7190889120101929\n",
      "Epoch: 50 | Loss: 2.051546096801758 | w_1: 0.47818368673324585 | w_2: 1.721184253692627\n",
      "Epoch: 51 | Loss: 2.0210583209991455 | w_1: 0.4671584367752075 | w_2: 1.7232639789581299\n",
      "Epoch: 52 | Loss: 1.9910224676132202 | w_1: 0.45621535181999207 | w_2: 1.7253278493881226\n",
      "Epoch: 53 | Loss: 1.961432695388794 | w_1: 0.44535401463508606 | w_2: 1.7273768186569214\n",
      "Epoch: 54 | Loss: 1.9322824478149414 | w_1: 0.43457353115081787 | w_2: 1.7294100522994995\n",
      "Epoch: 55 | Loss: 1.9035654067993164 | w_1: 0.42387351393699646 | w_2: 1.7314285039901733\n",
      "Epoch: 56 | Loss: 1.8752772808074951 | w_1: 0.4132533073425293 | w_2: 1.7334316968917847\n",
      "Epoch: 57 | Loss: 1.847405195236206 | w_1: 0.40271222591400146 | w_2: 1.7354196310043335\n",
      "Epoch: 58 | Loss: 1.819947600364685 | w_1: 0.3922499120235443 | w_2: 1.7373932600021362\n",
      "Epoch: 59 | Loss: 1.7929010391235352 | w_1: 0.381865531206131 | w_2: 1.739351749420166\n",
      "Epoch: 60 | Loss: 1.7662551403045654 | w_1: 0.37155869603157043 | w_2: 1.7412960529327393\n",
      "Epoch: 61 | Loss: 1.7400058507919312 | w_1: 0.3613286316394806 | w_2: 1.7432254552841187\n",
      "Epoch: 62 | Loss: 1.7141444683074951 | w_1: 0.3511749505996704 | w_2: 1.7451406717300415\n",
      "Epoch: 63 | Loss: 1.6886718273162842 | w_1: 0.341096967458725 | w_2: 1.7470417022705078\n",
      "Epoch: 64 | Loss: 1.663575530052185 | w_1: 0.3310941755771637 | w_2: 1.748928427696228\n",
      "Epoch: 65 | Loss: 1.6388533115386963 | w_1: 0.3211659789085388 | w_2: 1.7508010864257812\n",
      "Epoch: 66 | Loss: 1.6144946813583374 | w_1: 0.311311811208725 | w_2: 1.7526596784591675\n",
      "Epoch: 67 | Loss: 1.5904988050460815 | w_1: 0.3015311658382416 | w_2: 1.7545044422149658\n",
      "Epoch: 68 | Loss: 1.5668623447418213 | w_1: 0.29182347655296326 | w_2: 1.7563356161117554\n",
      "Epoch: 69 | Loss: 1.543575406074524 | w_1: 0.28218814730644226 | w_2: 1.758152961730957\n",
      "Epoch: 70 | Loss: 1.5206345319747925 | w_1: 0.2726247012615204 | w_2: 1.7599568367004395\n",
      "Epoch: 71 | Loss: 1.4980374574661255 | w_1: 0.2631326913833618 | w_2: 1.7617474794387817\n",
      "Epoch: 72 | Loss: 1.4757730960845947 | w_1: 0.2537113428115845 | w_2: 1.7635241746902466\n",
      "Epoch: 73 | Loss: 1.453840970993042 | w_1: 0.24436034262180328 | w_2: 1.765288233757019\n",
      "Epoch: 74 | Loss: 1.432237148284912 | w_1: 0.2350790649652481 | w_2: 1.7670389413833618\n",
      "Epoch: 75 | Loss: 1.4109532833099365 | w_1: 0.22586700320243835 | w_2: 1.768776535987854\n",
      "Epoch: 76 | Loss: 1.389981746673584 | w_1: 0.21672362089157104 | w_2: 1.7705010175704956\n",
      "Epoch: 77 | Loss: 1.369321584701538 | w_1: 0.20764845609664917 | w_2: 1.7722127437591553\n",
      "Epoch: 78 | Loss: 1.3489738702774048 | w_1: 0.19864103198051453 | w_2: 1.773911952972412\n",
      "Epoch: 79 | Loss: 1.3289241790771484 | w_1: 0.18970070779323578 | w_2: 1.7755980491638184\n",
      "Epoch: 80 | Loss: 1.3091728687286377 | w_1: 0.1808270961046219 | w_2: 1.7772717475891113\n",
      "Epoch: 81 | Loss: 1.2897166013717651 | w_1: 0.17201969027519226 | w_2: 1.778933048248291\n",
      "Epoch: 82 | Loss: 1.2705495357513428 | w_1: 0.16327796876430511 | w_2: 1.7805818319320679\n",
      "Epoch: 83 | Loss: 1.25166654586792 | w_1: 0.15460145473480225 | w_2: 1.7822184562683105\n",
      "Epoch: 84 | Loss: 1.233066201210022 | w_1: 0.14598968625068665 | w_2: 1.783842921257019\n",
      "Epoch: 85 | Loss: 1.2147419452667236 | w_1: 0.13744212687015533 | w_2: 1.7854552268981934\n",
      "Epoch: 86 | Loss: 1.1966872215270996 | w_1: 0.12895826995372772 | w_2: 1.787055253982544\n",
      "Epoch: 87 | Loss: 1.1789003610610962 | w_1: 0.12053774297237396 | w_2: 1.788643479347229\n",
      "Epoch: 88 | Loss: 1.1613801717758179 | w_1: 0.11218006908893585 | w_2: 1.790220022201538\n",
      "Epoch: 89 | Loss: 1.1441218852996826 | w_1: 0.10388471186161041 | w_2: 1.7917847633361816\n",
      "Epoch: 90 | Loss: 1.1271164417266846 | w_1: 0.09565116465091705 | w_2: 1.7933375835418701\n",
      "Epoch: 91 | Loss: 1.1103665828704834 | w_1: 0.0874791145324707 | w_2: 1.7948791980743408\n",
      "Epoch: 92 | Loss: 1.093865990638733 | w_1: 0.07936801016330719 | w_2: 1.796409249305725\n",
      "Epoch: 93 | Loss: 1.0776091814041138 | w_1: 0.07131728529930115 | w_2: 1.7979276180267334\n",
      "Epoch: 94 | Loss: 1.0615931749343872 | w_1: 0.06332667917013168 | w_2: 1.799434781074524\n",
      "Epoch: 95 | Loss: 1.0458157062530518 | w_1: 0.055395711213350296 | w_2: 1.8009308576583862\n",
      "Epoch: 96 | Loss: 1.0302761793136597 | w_1: 0.04752388596534729 | w_2: 1.8024157285690308\n",
      "Epoch: 97 | Loss: 1.0149630308151245 | w_1: 0.03971068188548088 | w_2: 1.803889274597168\n",
      "Epoch: 98 | Loss: 0.9998787045478821 | w_1: 0.03195581212639809 | w_2: 1.805351972579956\n",
      "Epoch: 99 | Loss: 0.9850174784660339 | w_1: 0.024258777499198914 | w_2: 1.806803822517395\n",
      "Epoch: 100 | Loss: 0.970378041267395 | w_1: 0.016619160771369934 | w_2: 1.8082448244094849\n"
     ]
    }
   ],
   "source": [
    "# In this method, we learn the dataset multiple times (called epochs)\n",
    "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
    "\n",
    "alpha = 0.001 # Let us set learning rate as 0.01\n",
    "weight_list_1 = []\n",
    "weight_list_2 = []\n",
    "loss_list=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in zip(x_data, y_data):\n",
    "        \n",
    "        #implement forward pass, compute loss and gradients for the weights and update weights\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        y_pred = quad_forward(x)\n",
    "        current_loss = loss(y_pred, y)\n",
    "        total_loss += current_loss\n",
    "        current_loss.backward()\n",
    "        \n",
    "        w_1.data = w_1.data - alpha * w_1.grad.item()\n",
    "        w_2.data = w_2.data - alpha * w_2.grad.item()\n",
    "        ## YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w_1.grad.data.zero_()\n",
    "        w_2.grad.data.zero_()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    avg_mse = total_loss / count        \n",
    "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w_1: {w_1.item()} | w_2: {w_2.item()}\")\n",
    "    weight_list_1.append(w_1)\n",
    "    weight_list_2.append(w_2)\n",
    "    loss_list.append(avg_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCxaFfeb4NN9"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=6$ after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "alIVupLO4NN9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Y Value for x=4 : 66\n",
      "Predicted Y Value before training :  42.0\n",
      "Predicted Y Value after training :  65.196533203125\n"
     ]
    }
   ],
   "source": [
    "y_pred_with_train = quad_forward(6)\n",
    "\n",
    "print(\"Actual Y Value for x=4 : 66\")\n",
    "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
    "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Task-03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kpb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "af5fc8c4b92f5405777167d61885746fb25c699dcf067d7c9e129d8b58c7734d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
