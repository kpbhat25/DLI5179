{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd0950ag4NNZ"
   },
   "source": [
    "### Task 03:\n",
    "\n",
    "In this task, you have to implement the Backpropagation method using Pytorch. This is particularly useful when the hypothesis function contains several weights.\n",
    "\n",
    "**Backpropagation**: Algorithm to caculate gradient for all the weights in the network with several weights. \n",
    "\n",
    "* It uses the `Chain Rule` to calcuate the gradient for multiple nodes at the same time. \n",
    "* In pytorch this is implemented using a `variable` data type and `loss.backward()` method to get the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2w0MM9204NNh"
   },
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st0vmvel4NNn"
   },
   "source": [
    "## Preliminaries - Pytorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BUgr0pR4NNq",
    "outputId": "f61783ca-70c0-4085-e209-3de30bf28733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([-0.0123,  0.2534,  0.8436, -0.8071, -0.1570])\n",
      "The size of the to_tensor:  torch.Size([5])\n",
      "The size of the to_tensor:  torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor\n",
    "\n",
    "# zero tensor\n",
    "zeros = torch.zeros(5)\n",
    "print(zeros)\n",
    "# ones\n",
    "ones = torch.ones(5)\n",
    "print(ones)\n",
    "# random normal\n",
    "random = torch.randn(5)\n",
    "print(random)\n",
    "\n",
    "\n",
    "# creating tensors from list and/or numpy arrays\n",
    "my_list = [0.0, 1.0, 2.0, 3.0, 4.0]\n",
    "to_tensor = torch.Tensor(my_list)\n",
    "print(\"The size of the to_tensor: \", to_tensor.size())\n",
    "\n",
    "my_array = np.array(my_list) # or\n",
    "to_tensor = torch.tensor(my_array)\n",
    "to_tensor = torch.from_numpy(my_array)\n",
    "print(\"The size of the to_tensor: \", to_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Js4p4btA4VvG",
    "outputId": "0dded7c7-ce10-465d-a38a-f1fdc57dc32f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1920, -0.9857,  0.0202],\n",
      "        [-0.4803,  0.4148, -0.1485],\n",
      "        [ 1.8382, -0.8884,  0.8270]])\n",
      "tensor([[[ 0.5306, -0.7220,  0.5482],\n",
      "         [-0.2700, -1.2972, -0.8821],\n",
      "         [-0.1850, -0.7852, -0.8710]],\n",
      "\n",
      "        [[-1.6633,  0.3835, -0.5511],\n",
      "         [ 0.2636, -0.2619,  0.6295],\n",
      "         [ 1.0871, -1.3955, -0.8907]],\n",
      "\n",
      "        [[ 1.2143,  0.7296,  0.0511],\n",
      "         [-0.2637,  0.1045,  0.5383],\n",
      "         [-0.4690, -0.6278, -0.7048]]])\n"
     ]
    }
   ],
   "source": [
    "# multi dimenstional tensors\n",
    "\n",
    "# 2D\n",
    "two_dim = torch.randn((3, 3))\n",
    "print(two_dim)\n",
    "# 3D \n",
    "three_dim = torch.randn((3, 3, 3))\n",
    "print(three_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRA0Icia4ani",
    "outputId": "417b5749-4ed9-4a7f-9a8c-73fda0cdccd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3, 3])\n",
      "tensor([ 0.1920, -0.4803,  1.8382])\n",
      "tensor([ 0.1920, -0.9857,  0.0202])\n"
     ]
    }
   ],
   "source": [
    "# tensor shapes and axes\n",
    "\n",
    "print(zeros.shape)\n",
    "print(two_dim.shape)\n",
    "print(three_dim.shape)\n",
    "\n",
    "# zeroth axis - rows\n",
    "print(two_dim[:, 0])\n",
    "# first axis - columns\n",
    "print(two_dim[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4CFwt4R_4iEq",
    "outputId": "0f4c05c4-3aae-4f5b-88e0-9d4f355d4f6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1920, -0.9857],\n",
      "        [-0.4803,  0.4148],\n",
      "        [ 1.8382, -0.8884]])\n",
      "tensor([[ 0.1920, -0.9857,  0.0202],\n",
      "        [-0.4803,  0.4148, -0.1485]])\n"
     ]
    }
   ],
   "source": [
    "print(two_dim[:, 0:2])\n",
    "print(two_dim[0:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJA1f5_F5Acn",
    "outputId": "ac3b4415-a3c6-4869-f2f5-f189af30c858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Shape :  torch.Size([2, 3])\n",
      "Resized Tensor Shape :  torch.Size([3, 2])\n",
      "Resized Tensor Shape :  torch.Size([3, 2])\n",
      "Flattened Tensor Shape :  torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "rand_tensor = torch.randn(2,3)\n",
    "print(\"Tensor Shape : \" , rand_tensor.shape)\n",
    "resized_tensor = rand_tensor.reshape(3,2)\n",
    "print(\"Resized Tensor Shape : \" , resized_tensor.shape) # or\n",
    "resized_tensor = rand_tensor.reshape(3,-1)\n",
    "print(\"Resized Tensor Shape : \" , resized_tensor.shape)\n",
    "flattened_tensor = rand_tensor.reshape(-1)\n",
    "print(\"Flattened Tensor Shape : \" , flattened_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ismMPyM4NNw"
   },
   "source": [
    "Determine the derivative of $y = 2x^{3} + x$ at $x = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ic-ZkAPC4NNx",
    "outputId": "4e7597fc-5f5c-46d1-9f3c-5af0f4d276f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of Y at x=1 :  tensor(3., grad_fn=<AddBackward0>)\n",
      "Derivative of Y wrt x at x=1 :  tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "y = 2 * (x ** 3) + x\n",
    "y.backward()\n",
    "print(\"Value of Y at x=1 : \" , y)\n",
    "print(\"Derivative of Y wrt x at x=1 : \" , x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gp-J-JDA4NNy"
   },
   "source": [
    "### Task 03 - a\n",
    "Determine the partial derivative of $y = uv + u^{2}$ at $u=1$ and $v=2$ with respect to $u$ and $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "N7ShF6wf4NNz"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "u = torch.tensor(1.0, requires_grad = True)\n",
    "v = torch.tensor(2.0, requires_grad = True)\n",
    "y = u*v + u**2\n",
    "y.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of y at u=1, v=2 :  tensor(3., grad_fn=<AddBackward0>)\n",
      "Partial Derivative of y wrt u :  tensor(4.)\n",
      "Partial Derivative of y wrt v :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# YOUR CODE ends HERE\n",
    "print(\"Value of y at u=1, v=2 : \" , y)\n",
    "print(\"Partial Derivative of y wrt u : \" , u.grad)\n",
    "print(\"Partial Derivative of y wrt v : \" , v.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jvHyKE4NN0"
   },
   "source": [
    "#### Hypothesis Function and Loss Function\n",
    "\n",
    "$y = x * w + b$\n",
    "\n",
    "$loss =(\\hat{y}-y)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7dU0O9E4NN0"
   },
   "source": [
    "Let us make use of a randomly-created sample dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "6tUXuGub4NN1"
   },
   "outputs": [],
   "source": [
    "#sample-dataset\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI84dQfW4NN1"
   },
   "source": [
    "## Task: 03 - b\n",
    "Declare pytorch tensors for weight and bias and implement the forward and loss function of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ZQaYqClT4NN1"
   },
   "outputs": [],
   "source": [
    "# Define w = 1 and b = -1 for y = wx + b\n",
    "# Note that w,b are learnable paramteter \n",
    "# i.e., you are going to take the derivative of the tensor(s).\n",
    "# YOUR CODE STARTS HERE\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "b = torch.tensor(-1.0, requires_grad = True)\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "assert w.item() == 1\n",
    "assert b.item() == -1\n",
    "assert w.requires_grad == True\n",
    "assert b.requires_grad == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zRVK-4tr4NN3"
   },
   "outputs": [],
   "source": [
    "#forward function to calculate y_pred for a given x according to the linear model defined above\n",
    "def forward(x):\n",
    "    #implement the forward model to compute y_pred as w*x + b\n",
    "    ## YOUR CODE STARTS HERE\n",
    "    return ((w*x) + b)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE\n",
    "\n",
    "#loss-function to compute the mean-squared error between y_pred and y_actual\n",
    "def loss(y_pred, y_actual):\n",
    "    #calculate the mean-squared-error between y_pred and y_actual\n",
    "    ## YOUR CODE STARTS HERE\n",
    "    return ((y_pred- y_actual)**2)\n",
    "\n",
    "    ## YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3BAF5aQ4NN3"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=4$ without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "EPAgSpvE4NN4"
   },
   "outputs": [],
   "source": [
    "y_pred_without_train = forward(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BO6QOWq4NN4"
   },
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "VUgLcPrG4NN4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 8.32815933227539 | w: 1.368575930595398\n",
      "Epoch: 2 | Loss: 4.635132312774658 | w: 1.641068696975708\n",
      "Epoch: 3 | Loss: 2.6127521991729736 | w: 1.842525839805603\n",
      "Epoch: 4 | Loss: 1.5045195817947388 | w: 1.991465449333191\n",
      "Epoch: 5 | Loss: 0.8966817855834961 | w: 2.1015784740448\n",
      "Epoch: 6 | Loss: 0.5628984570503235 | w: 2.182986259460449\n",
      "Epoch: 7 | Loss: 0.3793121576309204 | w: 2.2431719303131104\n",
      "Epoch: 8 | Loss: 0.2781200110912323 | w: 2.287667751312256\n",
      "Epoch: 9 | Loss: 0.22218374907970428 | w: 2.3205642700195312\n",
      "Epoch: 10 | Loss: 0.19114667177200317 | w: 2.3448848724365234\n"
     ]
    }
   ],
   "source": [
    "# In this method, we learn the dataset multiple times (called epochs)\n",
    "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
    "\n",
    "alpha = 0.01 # Let us set learning rate as 0.01\n",
    "weight_list = []\n",
    "loss_list=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in zip(x_data, y_data):\n",
    "        \n",
    "        #implement forward pass, compute loss and gradients for the weights and update weights\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        yPred = forward(x)            #forward\n",
    "        currentLoss = loss(yPred,y)     #loss\n",
    "        total_loss += currentLoss\n",
    "        currentLoss.backward()    #back\n",
    "\n",
    "        w.data = w.data - alpha * (w.grad.item()) #iterate/update weight\n",
    "        ## YOUR CODE ENDS HERE\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    avg_mse = total_loss / count        \n",
    "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w: {w.item()}\")\n",
    "    weight_list.append(w)\n",
    "    loss_list.append(avg_mse)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vfWzjw14NN5"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=4$ after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "KiQzppwU4NN6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Y Value for x=4 : 8\n",
      "Predicted Y Value before training :  3.0\n",
      "Predicted Y Value after training :  8.379539489746094\n"
     ]
    }
   ],
   "source": [
    "y_pred_with_train = forward(4)\n",
    "\n",
    "print(\"Actual Y Value for x=4 : 8\")\n",
    "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
    "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfsMg3xe4NN6"
   },
   "source": [
    "## Task: 03 - c\n",
    "Repeat **Task:03 - b** for the quadratic model defined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvN4ZfIt4NN6"
   },
   "source": [
    "#### Using backward propagation for quadratic model\n",
    "\n",
    "$\\hat{y} = x^2*w_{2} + x*w_{1}$\n",
    "\n",
    "$loss = (\\hat{y}-y)^2$\n",
    "\n",
    "* Using Dummy values of x and y\n",
    "\n",
    "`x = 1,2,3,4,5`\n",
    "`y = 1,6,15,28,45`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "pKxTLgYc4NN7"
   },
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "y_data = [1.0, 6.0, 15.0, 28, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0tvvovED4NN7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the given dataset\n",
    "plt.plot(x_data,y_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "nodeeWDX4NN7"
   },
   "outputs": [],
   "source": [
    "# Initialize w2 and w1 with randon values\n",
    "w_1 = torch.tensor([1.0], requires_grad=True)\n",
    "w_2 = torch.tensor([1.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4DZxKgss4NN7"
   },
   "outputs": [],
   "source": [
    "# Quadratic forward pass based on the function above. Taking b as zero for now\n",
    "def quad_forward(x):\n",
    "    return w_2*(x**2)+w_1*x\n",
    "\n",
    "# Loss fucntion as per the defination above\n",
    "def loss(y_pred,y_actual):\n",
    "    return (y_pred-y_actual)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKlMp8sW4NN8"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=6$ without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YiVbYy1P4NN8"
   },
   "outputs": [],
   "source": [
    "y_pred_without_train = quad_forward(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfUGvo2F4NN8"
   },
   "source": [
    "Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "04CX2FJX4NN9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 19.670841217041016 | w_1: 1.1621068716049194 | w_2: 1.7099769115447998\n",
      "Epoch: 2 | Loss: 6.430711269378662 | w_1: 1.115532636642456 | w_2: 1.610548496246338\n",
      "Epoch: 3 | Loss: 4.341702938079834 | w_1: 1.099595308303833 | w_2: 1.629080057144165\n",
      "Epoch: 4 | Loss: 4.463932037353516 | w_1: 1.0794039964675903 | w_2: 1.6303775310516357\n",
      "Epoch: 5 | Loss: 4.349801063537598 | w_1: 1.060043454170227 | w_2: 1.6341499090194702\n",
      "Epoch: 6 | Loss: 4.273285865783691 | w_1: 1.0407702922821045 | w_2: 1.6375246047973633\n",
      "Epoch: 7 | Loss: 4.193112373352051 | w_1: 1.0216909646987915 | w_2: 1.6409205198287964\n",
      "Epoch: 8 | Loss: 4.115159511566162 | w_1: 1.0027880668640137 | w_2: 1.644276738166809\n",
      "Epoch: 9 | Loss: 4.038552284240723 | w_1: 0.9840622544288635 | w_2: 1.6476027965545654\n",
      "Epoch: 10 | Loss: 3.9633827209472656 | w_1: 0.9655115008354187 | w_2: 1.65089750289917\n",
      "Epoch: 11 | Loss: 3.8896148204803467 | w_1: 0.9471341967582703 | w_2: 1.654161810874939\n",
      "Epoch: 12 | Loss: 3.817220687866211 | w_1: 0.928928554058075 | w_2: 1.6573951244354248\n",
      "Epoch: 13 | Loss: 3.7461726665496826 | w_1: 0.9108932614326477 | w_2: 1.660598635673523\n",
      "Epoch: 14 | Loss: 3.6764445304870605 | w_1: 0.8930264711380005 | w_2: 1.6637717485427856\n",
      "Epoch: 15 | Loss: 3.6080145835876465 | w_1: 0.8753268718719482 | w_2: 1.6669154167175293\n",
      "Epoch: 16 | Loss: 3.5408616065979004 | w_1: 0.8577927947044373 | w_2: 1.670029878616333\n",
      "Epoch: 17 | Loss: 3.4749622344970703 | w_1: 0.8404226303100586 | w_2: 1.6731152534484863\n",
      "Epoch: 18 | Loss: 3.4102847576141357 | w_1: 0.8232148885726929 | w_2: 1.6761715412139893\n",
      "Epoch: 19 | Loss: 3.3468120098114014 | w_1: 0.8061680793762207 | w_2: 1.6791993379592896\n",
      "Epoch: 20 | Loss: 3.2845165729522705 | w_1: 0.7892804741859436 | w_2: 1.6821985244750977\n",
      "Epoch: 21 | Loss: 3.2233805656433105 | w_1: 0.7725509405136108 | w_2: 1.6851699352264404\n",
      "Epoch: 22 | Loss: 3.163391590118408 | w_1: 0.7559778690338135 | w_2: 1.688113808631897\n",
      "Epoch: 23 | Loss: 3.1045126914978027 | w_1: 0.7395595908164978 | w_2: 1.6910297870635986\n",
      "Epoch: 24 | Loss: 3.04672908782959 | w_1: 0.7232949137687683 | w_2: 1.6939187049865723\n",
      "Epoch: 25 | Loss: 2.990025758743286 | w_1: 0.7071822285652161 | w_2: 1.6967805624008179\n",
      "Epoch: 26 | Loss: 2.934373140335083 | w_1: 0.6912202835083008 | w_2: 1.6996155977249146\n",
      "Epoch: 27 | Loss: 2.8797547817230225 | w_1: 0.6754074692726135 | w_2: 1.7024239301681519\n",
      "Epoch: 28 | Loss: 2.8261539936065674 | w_1: 0.659742534160614 | w_2: 1.7052063941955566\n",
      "Epoch: 29 | Loss: 2.773554563522339 | w_1: 0.6442241668701172 | w_2: 1.7079628705978394\n",
      "Epoch: 30 | Loss: 2.7219345569610596 | w_1: 0.6288508772850037 | w_2: 1.7106932401657104\n",
      "Epoch: 31 | Loss: 2.6712708473205566 | w_1: 0.6136212944984436 | w_2: 1.7133980989456177\n",
      "Epoch: 32 | Loss: 2.621551990509033 | w_1: 0.5985341668128967 | w_2: 1.7160779237747192\n",
      "Epoch: 33 | Loss: 2.572758436203003 | w_1: 0.5835880637168884 | w_2: 1.7187325954437256\n",
      "Epoch: 34 | Loss: 2.5248758792877197 | w_1: 0.5687816739082336 | w_2: 1.7213623523712158\n",
      "Epoch: 35 | Loss: 2.4778826236724854 | w_1: 0.5541138052940369 | w_2: 1.7239676713943481\n",
      "Epoch: 36 | Loss: 2.4317634105682373 | w_1: 0.5395830273628235 | w_2: 1.7265485525131226\n",
      "Epoch: 37 | Loss: 2.3865020275115967 | w_1: 0.5251880884170532 | w_2: 1.7291052341461182\n",
      "Epoch: 38 | Loss: 2.342083692550659 | w_1: 0.510927677154541 | w_2: 1.7316380739212036\n",
      "Epoch: 39 | Loss: 2.298491954803467 | w_1: 0.4968006908893585 | w_2: 1.7341471910476685\n",
      "Epoch: 40 | Loss: 2.2557120323181152 | w_1: 0.4828057885169983 | w_2: 1.736633062362671\n",
      "Epoch: 41 | Loss: 2.2137269973754883 | w_1: 0.46894168853759766 | w_2: 1.7390953302383423\n",
      "Epoch: 42 | Loss: 2.1725239753723145 | w_1: 0.4552072286605835 | w_2: 1.74153470993042\n",
      "Epoch: 43 | Loss: 2.1320881843566895 | w_1: 0.4416012763977051 | w_2: 1.743951439857483\n",
      "Epoch: 44 | Loss: 2.092405319213867 | w_1: 0.4281224012374878 | w_2: 1.7463454008102417\n",
      "Epoch: 45 | Loss: 2.053462028503418 | w_1: 0.41476961970329285 | w_2: 1.7487173080444336\n",
      "Epoch: 46 | Loss: 2.0152411460876465 | w_1: 0.4015416204929352 | w_2: 1.7510664463043213\n",
      "Epoch: 47 | Loss: 1.977731704711914 | w_1: 0.3884373605251312 | w_2: 1.7533941268920898\n",
      "Epoch: 48 | Loss: 1.9409242868423462 | w_1: 0.3754555881023407 | w_2: 1.755699872970581\n",
      "Epoch: 49 | Loss: 1.9047966003417969 | w_1: 0.36259517073631287 | w_2: 1.757983922958374\n",
      "Epoch: 50 | Loss: 1.869340181350708 | w_1: 0.34985506534576416 | w_2: 1.7602466344833374\n",
      "Epoch: 51 | Loss: 1.8345476388931274 | w_1: 0.3372340798377991 | w_2: 1.7624883651733398\n",
      "Epoch: 52 | Loss: 1.8004045486450195 | w_1: 0.32473108172416687 | w_2: 1.764709234237671\n",
      "Epoch: 53 | Loss: 1.766897201538086 | w_1: 0.3123449683189392 | w_2: 1.766909122467041\n",
      "Epoch: 54 | Loss: 1.7340103387832642 | w_1: 0.3000746965408325 | w_2: 1.7690885066986084\n",
      "Epoch: 55 | Loss: 1.701735258102417 | w_1: 0.28791916370391846 | w_2: 1.7712475061416626\n",
      "Epoch: 56 | Loss: 1.6700595617294312 | w_1: 0.2758772373199463 | w_2: 1.7733861207962036\n",
      "Epoch: 57 | Loss: 1.6389776468276978 | w_1: 0.2639479637145996 | w_2: 1.7755051851272583\n",
      "Epoch: 58 | Loss: 1.6084731817245483 | w_1: 0.2521301805973053 | w_2: 1.777604103088379\n",
      "Epoch: 59 | Loss: 1.578535795211792 | w_1: 0.24042290449142456 | w_2: 1.7796835899353027\n",
      "Epoch: 60 | Loss: 1.549156665802002 | w_1: 0.22882501780986786 | w_2: 1.7817434072494507\n",
      "Epoch: 61 | Loss: 1.520322322845459 | w_1: 0.21733567118644714 | w_2: 1.78378427028656\n",
      "Epoch: 62 | Loss: 1.492027997970581 | w_1: 0.2059536725282669 | w_2: 1.7858058214187622\n",
      "Epoch: 63 | Loss: 1.4642534255981445 | w_1: 0.19467811286449432 | w_2: 1.7878084182739258\n",
      "Epoch: 64 | Loss: 1.4370014667510986 | w_1: 0.1835079789161682 | w_2: 1.7897924184799194\n",
      "Epoch: 65 | Loss: 1.4102556705474854 | w_1: 0.17244228720664978 | w_2: 1.7917578220367432\n",
      "Epoch: 66 | Loss: 1.3840065002441406 | w_1: 0.16148003935813904 | w_2: 1.7937047481536865\n",
      "Epoch: 67 | Loss: 1.358245849609375 | w_1: 0.15062032639980316 | w_2: 1.7956336736679077\n",
      "Epoch: 68 | Loss: 1.3329664468765259 | w_1: 0.13986217975616455 | w_2: 1.7975444793701172\n",
      "Epoch: 69 | Loss: 1.3081586360931396 | w_1: 0.12920458614826202 | w_2: 1.7994375228881836\n",
      "Epoch: 70 | Loss: 1.2838106155395508 | w_1: 0.11864663660526276 | w_2: 1.8013126850128174\n",
      "Epoch: 71 | Loss: 1.2599154710769653 | w_1: 0.10818740725517273 | w_2: 1.8031704425811768\n",
      "Epoch: 72 | Loss: 1.2364667654037476 | w_1: 0.09782599657773972 | w_2: 1.8050107955932617\n",
      "Epoch: 73 | Loss: 1.2134513854980469 | w_1: 0.08756142854690552 | w_2: 1.8068337440490723\n",
      "Epoch: 74 | Loss: 1.190863847732544 | w_1: 0.07739287614822388 | w_2: 1.8086398839950562\n",
      "Epoch: 75 | Loss: 1.1687015295028687 | w_1: 0.067319355905056 | w_2: 1.8104290962219238\n",
      "Epoch: 76 | Loss: 1.1469495296478271 | w_1: 0.05734003335237503 | w_2: 1.8122016191482544\n",
      "Epoch: 77 | Loss: 1.1256020069122314 | w_1: 0.04745401814579964 | w_2: 1.8139575719833374\n",
      "Epoch: 78 | Loss: 1.1046502590179443 | w_1: 0.0376603789627552 | w_2: 1.8156967163085938\n",
      "Epoch: 79 | Loss: 1.0840911865234375 | w_1: 0.02795841544866562 | w_2: 1.817420244216919\n",
      "Epoch: 80 | Loss: 1.0639140605926514 | w_1: 0.018347082659602165 | w_2: 1.8191272020339966\n",
      "Epoch: 81 | Loss: 1.0441125631332397 | w_1: 0.008825712837278843 | w_2: 1.820818543434143\n",
      "Epoch: 82 | Loss: 1.0246798992156982 | w_1: -0.0006067296490073204 | w_2: 1.8224937915802002\n",
      "Epoch: 83 | Loss: 1.0056073665618896 | w_1: -0.009950952604413033 | w_2: 1.8241534233093262\n",
      "Epoch: 84 | Loss: 0.9868906736373901 | w_1: -0.019207805395126343 | w_2: 1.8257975578308105\n",
      "Epoch: 85 | Loss: 0.9685209393501282 | w_1: -0.028378114104270935 | w_2: 1.8274261951446533\n",
      "Epoch: 86 | Loss: 0.9504930377006531 | w_1: -0.03746265918016434 | w_2: 1.8290398120880127\n",
      "Epoch: 87 | Loss: 0.9328028559684753 | w_1: -0.046462297439575195 | w_2: 1.8306381702423096\n",
      "Epoch: 88 | Loss: 0.9154413342475891 | w_1: -0.05537775158882141 | w_2: 1.8322217464447021\n",
      "Epoch: 89 | Loss: 0.8984050750732422 | w_1: -0.06420981138944626 | w_2: 1.83379065990448\n",
      "Epoch: 90 | Loss: 0.8816840052604675 | w_1: -0.07295937091112137 | w_2: 1.8353445529937744\n",
      "Epoch: 91 | Loss: 0.8652714490890503 | w_1: -0.08162709325551987 | w_2: 1.8368840217590332\n",
      "Epoch: 92 | Loss: 0.8491684198379517 | w_1: -0.09021376073360443 | w_2: 1.838409185409546\n",
      "Epoch: 93 | Loss: 0.8333638906478882 | w_1: -0.09872013330459595 | w_2: 1.8399202823638916\n",
      "Epoch: 94 | Loss: 0.8178545832633972 | w_1: -0.10714709758758545 | w_2: 1.8414167165756226\n",
      "Epoch: 95 | Loss: 0.802628219127655 | w_1: -0.11549517512321472 | w_2: 1.8428994417190552\n",
      "Epoch: 96 | Loss: 0.7876907587051392 | w_1: -0.12376518547534943 | w_2: 1.8443684577941895\n",
      "Epoch: 97 | Loss: 0.773030161857605 | w_1: -0.13195794820785522 | w_2: 1.8458234071731567\n",
      "Epoch: 98 | Loss: 0.7586407661437988 | w_1: -0.14007404446601868 | w_2: 1.8472650051116943\n",
      "Epoch: 99 | Loss: 0.7445210218429565 | w_1: -0.14811421930789948 | w_2: 1.8486931324005127\n",
      "Epoch: 100 | Loss: 0.7306647300720215 | w_1: -0.15607918798923492 | w_2: 1.850108027458191\n"
     ]
    }
   ],
   "source": [
    "# In this method, we learn the dataset multiple times (called epochs)\n",
    "# Each time, the weight (w) gets updates using the graident decent algorithm based on weights of the previous epoch\n",
    "\n",
    "alpha = 0.0012 # Let us set learning rate as 0.01\n",
    "weight_list_1 = []\n",
    "weight_list_2 = []\n",
    "loss_list=[]\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    \n",
    "    for x, y in zip(x_data, y_data):\n",
    "        \n",
    "        #implement forward pass, compute loss and gradients for the weights and update weights\n",
    "        ## YOUR CODE STARTS HERE\n",
    "        y_pred = quad_forward(x)        #forward prop\n",
    "        current_loss = loss(y_pred, y)    #loss estimate\n",
    "        total_loss += current_loss        ##total loss \n",
    "        current_loss.backward()            #gradient \n",
    "        \n",
    "        w_1.data = w_1.data - (alpha * w_1.grad.item())  #update the weights\n",
    "        w_2.data = w_2.data - (alpha * w_2.grad.item())\n",
    "        ## YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Manually zero the gradients after updating weights\n",
    "        w_1.grad.data.zero_()\n",
    "        w_2.grad.data.zero_()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "    avg_mse = total_loss / count        \n",
    "    print(f\"Epoch: {epoch+1} | Loss: {avg_mse.item()} | w_1: {w_1.item()} | w_2: {w_2.item()}\")\n",
    "    weight_list_1.append(w_1)\n",
    "    weight_list_2.append(w_2)\n",
    "    loss_list.append(avg_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCxaFfeb4NN9"
   },
   "source": [
    "Calculate $y_{pred}$ for $x=6$ after training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "alIVupLO4NN9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Y Value for x=4 : 66\n",
      "Predicted Y Value before training :  42.0\n",
      "Predicted Y Value after training :  65.66741180419922\n"
     ]
    }
   ],
   "source": [
    "y_pred_with_train = quad_forward(6)\n",
    "\n",
    "print(\"Actual Y Value for x=4 : 66\")\n",
    "print(\"Predicted Y Value before training : \" , y_pred_without_train.item())\n",
    "print(\"Predicted Y Value after training : \" , y_pred_with_train.item())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Task-03.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kpb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "af5fc8c4b92f5405777167d61885746fb25c699dcf067d7c9e129d8b58c7734d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
