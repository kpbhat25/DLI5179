{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**EE5179:Deep Learning for Imaging**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Programming Assignment 3: MNIST Classification using RNN</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intoduction\n",
    "To demonstrate how to use recurrent neural network to predict the famous handwritten digits “MNIST”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries\n",
    "```\n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt \n",
    "from torchvision.datasets import MNIST\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam #Adam for GD\n",
    "import time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "The most important argument of DataLoader constructor is dataset, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:\n",
    "\n",
    "* map-style datasets,\n",
    "\n",
    "* iterable-style datasets. \\\n",
    "This is an iterable-style datasets, So we next use the function iter() and next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters:\n",
    "\n",
    "input_size — The number of expected features in the input x\n",
    "\n",
    "hidden_size — The number of features in the hidden state h\n",
    "\n",
    "num_layers — Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked RNN, with the second RNN taking in outputs of the first RNN and computing the final results. Default: 1\n",
    "\n",
    "nonlinearity — The non-linearity to use. Can be either ‘tanh’ or ‘relu’. Default: ‘tanh’\n",
    "\n",
    "bias — If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "\n",
    "batch_first — If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "\n",
    "dropout — If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "\n",
    "bidirectional — If True, becomes a bidirectional RNN. Default: False\n",
    "\n",
    "Ref : https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hidden_dim = 128\n",
    "input_dim = 28\n",
    "output_dim = 10 \n",
    "num_layers = 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters Used\n",
    "```\n",
    "learning_rate = 0.002\n",
    "num_epochs = 5\n",
    "lamb=0.01\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "optimizer1 = torch.optim.Adam(Vanilla_RNN(input_dim,hidden_dim,num_layers,output_dim).parameters(), lr=learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output\n",
    "```\n",
    "Epoch 1 out of 5 epochs are over\n",
    "Epoch 2 out of 5 epochs are over\n",
    "Epoch 3 out of 5 epochs are over\n",
    "Epoch 4 out of 5 epochs are over\n",
    "Epoch 5 out of 5 epochs are over\n",
    "Predicted tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "real-Label tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "Test Accuracy of the unregularized model on the 10000 test images: 93.08999999999999 %\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"outputVanilla.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vanilla_RNN.png\">\n",
    "Graphs for Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization is added in  the code , we can turn it on bu giving True as the last argument "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of running to 5 epochs\n",
    "``` \n",
    "Epoch 1 out of 5 epochs are over\n",
    "Epoch 2 out of 5 epochs are over\n",
    "Epoch 3 out of 5 epochs are over\n",
    "Epoch 4 out of 5 epochs are over\n",
    "Epoch 5 out of 5 epochs are over\n",
    "Predicted tensor([7, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "real-Label tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "Test Accuracy of the unregularized model on the 10000 test images: 96.98 %\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vanilla_LSTM.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vanilla_LSTMgraphs.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output :\n",
    "\n",
    "```\n",
    "Epoch 1 out of 5 epochs are over\n",
    "Epoch 2 out of 5 epochs are over\n",
    "Epoch 3 out of 5 epochs are over\n",
    "Epoch 4 out of 5 epochs are over\n",
    "Epoch 5 out of 5 epochs are over\n",
    "Predicted tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "real-Label tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "\n",
    "Test Accuracy of the unregularized model on the 10000 test images: 96.8 %\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GRU.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GRUOutput.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidrectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output \n",
    "```\n",
    "Epoch 1 out of 5 epochs are over\n",
    "Epoch 2 out of 5 epochs are over\n",
    "Epoch 3 out of 5 epochs are over\n",
    "Epoch 4 out of 5 epochs are over\n",
    "Epoch 5 out of 5 epochs are over\n",
    "Predicted tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "real-Label tensor([3, 4, 5, 6, 7, 8, 9, 0, 1])\n",
    "\n",
    "Test Accuracy of the unregularized model on the 10000 test images: 94.75 %\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"BidRNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- First, comparing the RNNs, LSTMs and GRU, we see that for this particular experiment, LSTMs and GRUs outperform the\n",
    "RNN while they train for just half the number of Epochs. Between GRUs and LSTMs there is not much different in the\n",
    "accuracies.\n",
    "- It is interesting to see that the GRU took longer to train. However, this trend did not carry forward on repeating the experiments.\n",
    "- We see that the one with a smaller hidden size takes slightly lesser to train.\n",
    "- We see that the bidirectional LSTM didn’t improve the accuracy by a lot while taking almost double the time to train.\n",
    "\n",
    "- The regularized LSTM trains much much faster compared to the other models and has a comparable accuracy too, due to\n",
    "the low value of the regularization coefficient.This brings to light the tradeoff between model complexity and computational\n",
    "complexit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Remembering the number at a particular index in a given sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You will need to create a function that can create a random input-output pair . Write a function that takes a sequence\n",
    "length L as input and returns a training sample to be fed to the RNN. For a given length K, a training sample is a\n",
    "2-tuple of input-x and output-y where input is a tensor of size L×10: Second dimension is 10 since each integer is\n",
    "represented as a one-hot vector of dimension=10 (since we consider only sequence of integers in [0-9]). Output is just\n",
    "a single number . Its the number at the Kth position which need to be remembered. So y is a tensor with just one\n",
    "element of size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def Random_creator(L):\n",
    "  #takes input of length L \n",
    "  return np.random.randint(0,10,L)\n",
    "\n",
    "def one_hot_encoded(x):\n",
    "  x_new = np.zeros((len(x),10))\n",
    "  x_new[np.arange(len(x)), x] = 1\n",
    "  return x_new\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Train 3 different RNN models: with hidden state size 2, 5, 10. For each of the experiments, you are required to\n",
    "show the plot of training error and prediction accuracy over the training progress. For each of the experiments,\n",
    "at the end of training, report the average prediction accuracy for the test set you created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q21.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Test Accuracy of the model with 2 hidden states  16.0 %\n",
    "Test Accuracy of the model with 5 hidden states 30.0 %\n",
    "Test Accuracy of the model with 10 hidden states 96.0 %\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "For Length 3\n",
    "Array: [4 9 9] Prediction at position 2: 9\n",
    "Array: [0 6 0] Prediction at position 2: 6\n",
    "Array: [2 4 5] Prediction at position 2: 4\n",
    "For Length 4\n",
    "Array: [2 1 5 2] Prediction at position 2: 1\n",
    "Array: [9 4 7 7] Prediction at position 2: 4\n",
    "Array: [8 1 4 0] Prediction at position 2: 1\n",
    "For Length 5\n",
    "Array: [6 4 1 6 4] Prediction at position 2: 4\n",
    "Array: [8 7 3 0 2] Prediction at position 2: 7\n",
    "Array: [7 8 4 3 6] Prediction at position 2: 8\n",
    "For Length 6\n",
    "Array: [0 1 5 5 1 9] Prediction at position 2: 1\n",
    "Array: [4 3 1 1 2 7] Prediction at position 2: 3\n",
    "Array: [3 0 2 3 7 6] Prediction at position 2: 0\n",
    "For Length 7\n",
    "Array: [9 1 3 0 6 2 4] Prediction at position 2: 1\n",
    "Array: [9 2 2 7 0 0 6] Prediction at position 2: 2\n",
    "Array: [6 1 6 0 0 8 0] Prediction at position 2: 1\n",
    "For Length 8\n",
    "Array: [2 9 0 9 4 9 9 5] Prediction at position 2: 9\n",
    "Array: [7 7 7 8 6 8 5 6] Prediction at position 2: 7\n",
    "Array: [2 2 5 3 6 3 3 2] Prediction at position 2: 2\n",
    "For Length 9\n",
    "Array: [8 3 2 4 3 7 3 7 2] Prediction at position 2: 3\n",
    "Array: [9 2 1 6 4 9 5 6 2] Prediction at position 2: 2\n",
    "Array: [2 1 0 4 4 6 8 0 1] Prediction at position 2: 1\n",
    "For Length 10\n",
    "Array: [8 5 5 3 0 1 7 5 9 7] Prediction at position 2: 5\n",
    "Array: [2 5 1 8 2 2 1 1 9 0] Prediction at position 2: 5\n",
    "Array: [7 5 4 4 7 0 3 2 1 7] Prediction at position 2: 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- We see that as the size of the hidden length increases, the capability to remember increases and therefore the accuracy also\n",
    "improves.\n",
    "- Amongst RNNs, LSTMs and GRUs, the latter performs the best as the inputs are all small length sequences. \n",
    "- Observing the predicted outputs, we can see that the trained models indeed remember the index very well.\n",
    "- When trained for lesser number of epochs prediction wasn't right , it learns more with learning \n",
    "- since the sequence is small its able to remember it and give the right output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Adding two binary strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q31.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q32.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Q33.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations \n",
    "- cross entropy loss performs marginally better than the MSE loss\n",
    "- More hidden layers ,more to learn , so increase in accuracy "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
