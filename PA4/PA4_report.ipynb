{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <center> EE5179 : Deep Learning for Imaging </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Programming Assignment 4: Autoencoders </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1 Comparing PCA and Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Encoder*:Flattened Image (28*28) → fc(512) → ReLU → fc(256) → ReLU → fc(128) → ReLU → fc(30) → hidden  \\\n",
    "*Decoder*: hidden → fc(128) →ReLU → fc(256) → ReLU → fc(784) → ReLU → → fc(784) →ReLU → reconstructed\n",
    "image (28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "batch_size = 256\n",
    "epochs =10\n",
    "learning_rate  = 0.001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training image shape:torch.Size([60000, 28, 28])\n",
    "Training Targets shape:torch.Size([60000])\n",
    "validation image shape:torch.Size([10000, 28, 28])\n",
    "validation Targets shape:torch.Size([10000])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"Data\" alt=\"Import Data\" src=\"datacheck.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do PCA on it and take only the first 30 eigenvalues with their corresponding eigenvectors \n",
    "```\n",
    "PCA with 30 principal components\n",
    "Reconstruction error made by PCA:  0.01805640184447451\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"pca_0.png\">\n",
    "<img  src=\"pca_1.png\">\n",
    "<img  src=\"pca_9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "project the data onto these eigenvectors and reconstruct them . Next , train an autoencoder with the following architecture\n",
    "```\n",
    "AE1(\n",
    "  (encoder): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
    "    (5): ReLU()\n",
    "    (6): Linear(in_features=128, out_features=30, bias=True)\n",
    "    (7): ReLU()\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Linear(in_features=30, out_features=128, bias=True)\n",
    "    (1): ReLU()\n",
    "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
    "    (3): ReLU()\n",
    "    (4): Linear(in_features=256, out_features=784, bias=True)\n",
    "    (5): ReLU()\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"ae1_0.png\"> \\\n",
    "<img  src=\"ae1_1.png\"> \\\n",
    "<img  src=\"ae1_9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"ae1_test.png\">\\\n",
    "<img  src=\"ae1_train.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE Reconstruction error for Vanilla AE is  **0.016686905175447464**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "* We see that the reconstruction accuracy of AE is much much lower than PCA when brought to the same scale which is what\n",
    "we’d expect.\n",
    "* The AE does a much better job at reconstructing the digit. The reconstructed digit is almost a copy of the original test digit\n",
    "but with a few missing pixels here and there. The contour of the digit also seems to be smoother and larger.\n",
    "* The AE performs much better than the PCA as it makes use of non-linear activation functions whereas PCA is a linear scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 Experimenting with hidden units of varying sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a standard auto-encoder with the following architecture:\n",
    "• fc(x)-fc(784) \\\n",
    "Here, x is the size of the hidden unit. \\\n",
    "The architecture consists of only a hidden layer and the output layer. Using x = [64, 128, 256], do the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "AE_Q2(\n",
    "  (encoder): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=64, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Linear(in_features=64, out_features=784, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    ")\n",
    "```\n",
    "\n",
    "```\n",
    "AE_Q2(\n",
    "  (encoder): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Linear(in_features=128, out_features=784, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    ")\n",
    "```\n",
    "```\n",
    "AE_Q2(\n",
    "  (encoder): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Linear(in_features=256, out_features=784, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Training and validation Loss for the 3 cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *64* \n",
    "<img  src=\"a2_64_train.png\"> \\\n",
    "<img  src=\"a2_64_test.png\"> \\\n",
    "<img  src=\"q2_64_0.png\"> \\\n",
    "<img  src=\"q2_64_5.png\"> \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *128* \n",
    "\n",
    "<img  src=\"q2_128_train.png\"> \\\n",
    "<img  src=\"q2_128_test.png\"> \\\n",
    "<img  src=\"q2_128_0.png\"> \\\n",
    "<img  src=\"q2_128_5.png\"> \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *256* \n",
    "<img  src=\"q2_256_train.png\"> \\\n",
    "<img  src=\"q2_256_test.png\"> \\\n",
    "<img  src=\"q2_256_0.png\"> \\\n",
    "<img  src=\"q2_256_5.png\"> \\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What outputs do you get if you pass a non-digit image \\\n",
    "Taken the image from the SKIMAGE - DATA \\\n",
    "Astronaut \\\n",
    "<img  src=\"astronaut.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"nondigit1.png\">\\\n",
    "<img  src=\"nondigit2.png\"> \n",
    "\n",
    "For rest of the imagees its been printed in the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On observing the output of the reconstructed image of Lena, we see that the AE has learnt a manifold which has a dark\n",
    "background. This is expected as the training data is also of the same form.\n",
    "* We see that a lot of the background is dark for the 256 neuron hidden layer case. On performing this experiment for the\n",
    "other two auto encoders, there is more and more portions visible as the hidden layer size goes lower. This could mean a lesser\n",
    "understanding of the manifold space by the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Sparse Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "AE_Q3(\n",
    "  (encoder): Sequential(\n",
    "    (0): Linear(in_features=784, out_features=1225, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder): Sequential(\n",
    "    (0): Linear(in_features=1225, out_features=784, bias=True)\n",
    "    (1): ReLU()\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average activation of overcomplete sparse AE with sparsity parameter **1e-06** is **0.27716308906674386** \n",
    "\n",
    "The average activation of overcomplete sparse AE with sparsity parameter **0.001** is **0.004904146178159863** \n",
    "\n",
    "The average activation of overcomplete sparse AE with sparsity parameter **0.1** is **0.0004624879096809309**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e-06 \\\n",
    "<img  src=\"q301.png\">\\\n",
    "<img  src=\"q302.png\">\\\n",
    "<img  src=\"q303.png\">\\\n",
    "<img  src=\"q304.png\">\\\n",
    "<img  src=\"q305.png\">\\\n",
    "<img  src=\"q309.png\">\\\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e-03 \\\n",
    "<img  src=\"q311.png\">\\\n",
    "<img  src=\"q312.png\">\\\n",
    "<img  src=\"q313.png\">\\\n",
    "<img  src=\"q314.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1e-06 \\\n",
    "<img  src=\"q321.png\">\\\n",
    "<img  src=\"q322.png\">\\\n",
    "<img  src=\"q323.png\">\\\n",
    "<img  src=\"q324.png\">\\\n",
    "<img  src=\"q325.png\">\\\n",
    "<img  src=\"q326.png\">\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to visualize the learned filters of this Sparse AutoEncoder as images. What difference do you observe in the\\\n",
    "structure of these filters from the ones you learned using the Standard AutoEncoder \\\n",
    "<img  src=\"q3.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that the average activation values are lower for a higher regularization and much higher for the standard AE. (note:\n",
    "difference may appear small, but it is scaled by an order of 104).\n",
    "* We see that the difference in the average activations for the last two regularizations are not a lot which is because both of them\n",
    "are high and they influence the loss function in a similar manner thus leading to a similar result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design a denoising autoencoder with just one hidden unit (Take hidden size as 256).\n",
    "1. What happens when you pass images corrupted with noise to the previously trained Standard Autoencoders (From Q2)\n",
    "? Compare it with Denoising autoencoders\n",
    "2. Change the noise level (typical values: 0.3, 0.5, 0.8, 0.9) and repeat the above experiments. What kind of variations\n",
    "do you observe in the results? (Both Visually and by MSE)\n",
    "3. Visualize the learned filters for Denoising Autoencoders. Compare it with that of Standard Autoencoders. What\n",
    "difference do you observe between them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "MSE for the  0.3 = 0.0033181174658238888 \n",
    "MSE for the  0.5 = 0.0038982063997536898 \n",
    "MSE for the  0.8 = 0.006269432138651609 \n",
    "MSE for the  0.9 = 0.006520132068544626\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.3** \\\n",
    "<img  src=\"q401.png\">\\\n",
    "<img  src=\"q402.png\">\\\n",
    "<img  src=\"q403.png\">\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.5** \\\n",
    "<img  src=\"q411.png\">\\\n",
    "<img  src=\"q412.png\">\\\n",
    "<img  src=\"q413.png\">\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.8** \\\n",
    "<img  src=\"q421.png\">\\\n",
    "<img  src=\"q422.png\">\\\n",
    "<img  src=\"q423.png\">\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**0.9** \\\n",
    "<img  src=\"q431.png\">\\\n",
    "<img  src=\"q432.png\">\\\n",
    "<img  src=\"q433.png\">\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "AE can also be implemented as fully convolutional networks with the decoder consisting of upsampling operations of any of \\\n",
    "these variants - i) Unpooling or ii) Unpooling + Deconvolution or iii) Deconvolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conv_AE_with_unpooling(\n",
    "  (encoder_conv1): Sequential(\n",
    "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (encoder_conv2): Sequential(\n",
    "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (encoder_conv3): Sequential(\n",
    "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (decoder_conv1): Sequential(\n",
    "    (0): Identity()\n",
    "  )\n",
    "  (decoder_conv2): Sequential(\n",
    "    (0): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder_conv3): Sequential(\n",
    "    (0): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img  src=\"q501.png\">\\\n",
    "<img  src=\"q502.png\">\\\n",
    "<img  src=\"q503.png\">\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conv_AE_with_deconv(\n",
    "  (encoder_conv1): Sequential(\n",
    "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (encoder_conv2): Sequential(\n",
    "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (encoder_conv3): Sequential(\n",
    "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (decoder_conv1): Sequential(\n",
    "    (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder_conv2): Sequential(\n",
    "    (0): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder_conv3): Sequential(\n",
    "    (0): ConvTranspose2d(8, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  src=\"q511.png\">\\\n",
    "<img  src=\"q512.png\">\\\n",
    "<img  src=\"q513.png\">\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conv_AE_with_deconv_unpool(\n",
    "  (encoder_conv1): Sequential(\n",
    "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (encoder_conv2): Sequential(\n",
    "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (encoder_conv3): Sequential(\n",
    "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
    "  )\n",
    "  (decoder_conv1): Sequential(\n",
    "    (0): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder_conv2): Sequential(\n",
    "    (0): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (decoder_conv3): Sequential(\n",
    "    (0): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "    (1): ReLU()\n",
    "  )\n",
    "  (unpool): MaxUnpool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0))\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img  src=\"q521.png\">\\\n",
    "<img  src=\"q522.png\">\\\n",
    "<img  src=\"q523.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img  src=\"q51.png\">\\\n",
    "<img  src=\"q52.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DLI')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00a1ae1e6deaa37076d141dc26cd1e058ef740ecaf1a0d690f5f0ccaa51e4cb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
